# Base URL for the OpenAI instance (default is https://api.openai.com/v1)
# OpenAI: https://api.openai.com/v1
# Ollama (example): http://localhost:11434/v1
# OpenRouter: https://openrouter.ai/api/v1
# Anthropic: https://api.anthropic.com/v1
# AWS Bedrock: set to "bedrock" to use AWS Bedrock
BASE_URL=

# For OpenAI: https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# For Anthropic: https://console.anthropic.com/account/keys
# For OpenRouter: https://openrouter.ai/keys
# For Ollama, no need to set this unless you specifically configured an API key
# For AWS Bedrock: not used (AWS credentials are used instead)
LLM_API_KEY=

# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# Even if using Anthropic or OpenRouter, you still need to set this for the embedding model.
# No need to set this if using Ollama.
OPENAI_API_KEY=

# AWS Bedrock Configuration
# For AWS Bedrock: Set the AWS access key ID from your IAM credentials
AWS_ACCESS_KEY_ID=

# For AWS Bedrock: Set the AWS secret access key from your IAM credentials
AWS_SECRET_ACCESS_KEY=

# For AWS Bedrock: Set the AWS region where the Bedrock model is deployed (e.g., us-east-1)
AWS_REGION=us-east-1

# For AWS Bedrock: Set the Claude model ID to use (default: anthropic.claude-3-7-sonnet-20250219-v1:0)
AWS_CLAUDE_MODEL_ID=anthropic.claude-3-7-sonnet-20250219-v1:0

# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# The LLM you want to use for the reasoner (o3-mini, R1, QwQ, etc.).
# Example: o3-mini
# Example: deepseek-r1:7b-8k
REASONER_MODEL=

# The LLM you want to use for the primary agent/coder.
# Example: gpt-4o-mini
# Example: qwen2.5:14b-instruct-8k
PRIMARY_MODEL=

# Embedding model you want to use
# Example for Ollama: nomic-embed-text
# Example for OpenAI: text-embedding-3-small
EMBEDDING_MODEL=